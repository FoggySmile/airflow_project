[2024-09-20T12:42:15.510+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [queued]>
[2024-09-20T12:42:15.528+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [queued]>
[2024-09-20T12:42:15.528+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2024-09-20T12:42:15.529+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 2
[2024-09-20T12:42:15.530+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2024-09-20T12:42:15.562+0000] {taskinstance.py:1300} INFO - Executing <Task(SparkSubmitOperator): spark_weekly_aggregate> on 2024-09-19 07:00:00+00:00
[2024-09-20T12:42:15.569+0000] {standard_task_runner.py:55} INFO - Started process 357 to run task
[2024-09-20T12:42:15.577+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'weekly_aggregate', 'spark_weekly_aggregate', 'scheduled__2024-09-19T07:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/weekly_aggregate_dag.py', '--cfg-path', '/tmp/tmpdafw_o82']
[2024-09-20T12:42:15.583+0000] {standard_task_runner.py:83} INFO - Job 2: Subtask spark_weekly_aggregate
[2024-09-20T12:42:15.623+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/settings.py:249 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-20T12:42:15.684+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-20T12:42:15.741+0000] {task_command.py:388} INFO - Running <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [running]> on host ae946b97fe07
[2024-09-20T12:42:15.879+0000] {taskinstance.py:1509} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=weekly_aggregate
AIRFLOW_CTX_TASK_ID=spark_weekly_aggregate
AIRFLOW_CTX_EXECUTION_DATE=2024-09-19T07:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-19T07:00:00+00:00
[2024-09-20T12:42:15.906+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-20T12:42:15.933+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19
[2024-09-20T12:42:16.114+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-09-20T12:42:16.115+0000] {spark_submit.py:490} INFO - JAVA_HOME is not set
[2024-09-20T12:42:16.132+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19. Error code is: 1.
[2024-09-20T12:42:16.138+0000] {taskinstance.py:1323} INFO - Marking task as UP_FOR_RETRY. dag_id=weekly_aggregate, task_id=spark_weekly_aggregate, execution_date=20240919T070000, start_date=20240920T124215, end_date=20240920T124216
[2024-09-20T12:42:16.162+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 2 for task spark_weekly_aggregate (Cannot execute: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19. Error code is: 1.; 357)
[2024-09-20T12:42:16.209+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2024-09-20T12:42:16.231+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-20T12:57:08.812+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [queued]>
[2024-09-20T12:57:08.840+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [queued]>
[2024-09-20T12:57:08.841+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2024-09-20T12:57:08.842+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 2
[2024-09-20T12:57:08.842+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2024-09-20T12:57:08.872+0000] {taskinstance.py:1300} INFO - Executing <Task(SparkSubmitOperator): spark_weekly_aggregate> on 2024-09-19 07:00:00+00:00
[2024-09-20T12:57:08.880+0000] {standard_task_runner.py:55} INFO - Started process 224 to run task
[2024-09-20T12:57:08.888+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'weekly_aggregate', 'spark_weekly_aggregate', 'scheduled__2024-09-19T07:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/weekly_aggregate_dag.py', '--cfg-path', '/tmp/tmpon_waq05']
[2024-09-20T12:57:08.893+0000] {standard_task_runner.py:83} INFO - Job 2: Subtask spark_weekly_aggregate
[2024-09-20T12:57:08.942+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/settings.py:249 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-20T12:57:09.012+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-20T12:57:09.099+0000] {task_command.py:388} INFO - Running <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [running]> on host ad1317ca320a
[2024-09-20T12:57:09.311+0000] {taskinstance.py:1509} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=weekly_aggregate
AIRFLOW_CTX_TASK_ID=spark_weekly_aggregate
AIRFLOW_CTX_EXECUTION_DATE=2024-09-19T07:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-19T07:00:00+00:00
[2024-09-20T12:57:09.335+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-20T12:57:09.353+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19
[2024-09-20T12:57:09.639+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-8-openjdk-amd64/bin/java: No such file or directory
[2024-09-20T12:57:09.641+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript
[2024-09-20T12:57:09.673+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19. Error code is: 1.
[2024-09-20T12:57:09.685+0000] {taskinstance.py:1323} INFO - Marking task as UP_FOR_RETRY. dag_id=weekly_aggregate, task_id=spark_weekly_aggregate, execution_date=20240919T070000, start_date=20240920T125708, end_date=20240920T125709
[2024-09-20T12:57:09.724+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 2 for task spark_weekly_aggregate (Cannot execute: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19. Error code is: 1.; 224)
[2024-09-20T12:57:09.770+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2024-09-20T12:57:09.808+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-20T13:31:08.018+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [queued]>
[2024-09-20T13:31:08.076+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [queued]>
[2024-09-20T13:31:08.078+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2024-09-20T13:31:08.079+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 2
[2024-09-20T13:31:08.080+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2024-09-20T13:31:08.181+0000] {taskinstance.py:1300} INFO - Executing <Task(SparkSubmitOperator): spark_weekly_aggregate> on 2024-09-19 07:00:00+00:00
[2024-09-20T13:31:08.201+0000] {standard_task_runner.py:55} INFO - Started process 216 to run task
[2024-09-20T13:31:08.223+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'weekly_aggregate', 'spark_weekly_aggregate', 'scheduled__2024-09-19T07:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/weekly_aggregate_dag.py', '--cfg-path', '/tmp/tmp4czglc69']
[2024-09-20T13:31:08.243+0000] {standard_task_runner.py:83} INFO - Job 3: Subtask spark_weekly_aggregate
[2024-09-20T13:31:08.356+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/settings.py:249 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-20T13:31:08.504+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-20T13:31:08.709+0000] {task_command.py:388} INFO - Running <TaskInstance: weekly_aggregate.spark_weekly_aggregate scheduled__2024-09-19T07:00:00+00:00 [running]> on host 6320e57075aa
[2024-09-20T13:31:09.201+0000] {taskinstance.py:1509} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=weekly_aggregate
AIRFLOW_CTX_TASK_ID=spark_weekly_aggregate
AIRFLOW_CTX_EXECUTION_DATE=2024-09-19T07:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-19T07:00:00+00:00
[2024-09-20T13:31:09.251+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-20T13:31:09.328+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19
[2024-09-20T13:31:24.862+0000] {spark_submit.py:490} INFO - Using properties file: null
[2024-09-20T13:31:25.895+0000] {spark_submit.py:490} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2024-09-20T13:31:25.902+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)
[2024-09-20T13:31:25.905+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)
[2024-09-20T13:31:25.909+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)
[2024-09-20T13:31:25.912+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)
[2024-09-20T13:31:25.915+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1094)
[2024-09-20T13:31:25.917+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1094)
[2024-09-20T13:31:25.920+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
[2024-09-20T13:31:25.921+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
[2024-09-20T13:31:25.924+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
[2024-09-20T13:31:25.925+0000] {spark_submit.py:490} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2024-09-20T13:31:26.036+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19. Error code is: 1.
[2024-09-20T13:31:26.060+0000] {taskinstance.py:1323} INFO - Marking task as UP_FOR_RETRY. dag_id=weekly_aggregate, task_id=spark_weekly_aggregate, execution_date=20240919T070000, start_date=20240920T133108, end_date=20240920T133126
[2024-09-20T13:31:26.141+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 3 for task spark_weekly_aggregate (Cannot execute: spark-submit --master yarn --name spark_weekly_aggregate --verbose --queue root.default /opt/***/dags/spark_aggregate.py 2024-09-19. Error code is: 1.; 216)
[2024-09-20T13:31:26.204+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2024-09-20T13:31:26.308+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
