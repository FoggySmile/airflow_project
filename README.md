## Еженедельная агрегация данных с использованием Apache Airflow и Apache Spark

Проект предназначен для выполнения еженедельной агрегации действий пользователей, хранящихся в CSV-файлах, с использованием Apache Airflow для оркестрации и Apache Spark для обработки данных.

### Запуск проекта

1. Клонирование репозитория

```
git clone https://github.com/FoggySmile/airflow_project.git
cd airflow_project
```

### Минимальная часть
Для проверки работы минимальной части проекта, введите команду:
```
 python3 script.py 2024-09-16
```
### Рекомендуемая часть

2. Настройка Docker

Убедитесь, что у вас установлены Docker и Docker Compose. Для сборки и запуска всех необходимых сервисов выполните команду:
```
docker-compose up --build
```

3. Настройка Airflow

Перейдите по адресу http://localhost:8080 в браузере для доступа к веб-интерфейсу Airflow. Если потребуется, создайте пользователя с административными правами.

4. Проверка DAG
Убедитесь, что DAG под названием weekly_aggregate виден в интерфейсе Airflow. Если он не активен, переключите его для активации.

5. Запуск DAG
Запустите DAG вручную через веб-интерфейс Airflow, нажав на кнопку запуска. Это запустит задачу Spark для агрегации данных.

6. Результаты (!не реализовано)
Агрегированные результаты будут сохранены в папке output в формате CSV с именем, соответствующим текущей дате.
